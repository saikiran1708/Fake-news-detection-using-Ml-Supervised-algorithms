{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60717cc-e562-4dae-955e-3c16f0c8b542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ROBIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ROBIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttrain's binary_error: 0.00360036\ttrain's auc: 0.999865\ttest's binary_error: 0.0042\ttest's auc: 0.999724\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttrain's binary_error: 0.00365037\ttrain's auc: 0.999845\ttest's binary_error: 0.0038\ttest's auc: 0.999753\n",
      "All models and vectorizer saved successfully.\n",
      "\n",
      "Model Accuracies:\n",
      "LightGBM: 0.9962\n",
      "Random Forest: 0.9940\n",
      "Logistic Regression: 0.9822\n",
      "Naive Bayes: 0.9306\n",
      "Decision Tree: 0.9948\n",
      "K-Nearest Neighbors: 0.7154\n",
      "AdaBoost: 0.9930\n",
      "SVM: 0.9910\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load datasets\n",
    "fake = pd.read_csv(r\"c:\\Users\\ROBIN\\Desktop\\FAKE.csv\")\n",
    "real = pd.read_csv(r\"c:\\Users\\ROBIN\\Desktop\\REAL.csv\")\n",
    "\n",
    "# Add labels and combine datasets\n",
    "fake['target'] = 'fake'\n",
    "real['target'] = 'real'\n",
    "data = pd.concat([fake, real]).reset_index(drop=True).sample(frac=1, random_state=42)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data.drop([\"date\", \"title\"], axis=1, inplace=True)\n",
    "\n",
    "# Text preprocessing function\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  \n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply text preprocessing\n",
    "data['cleaned_text'] = data['text'].fillna('').apply(clean_text)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['cleaned_text'], data['target'], test_size=0.2, stratify=data['target'], random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000, tokenizer=word_tokenize, token_pattern=None)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Dictionary to store models and accuracy\n",
    "models = {}\n",
    "accuracies = {}\n",
    "\n",
    "# LightGBM Model\n",
    "params = {\n",
    "    'objective': 'binary', 'boosting_type': 'gbdt', 'metric': ['binary_error', 'auc'],\n",
    "    'num_leaves': 63, 'learning_rate': 0.05, 'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8, 'bagging_freq': 5, 'verbose': -1, 'class_weight': 'balanced'\n",
    "}\n",
    "train_data = lgb.Dataset(X_train_tfidf, label=y_train_encoded)\n",
    "test_data = lgb.Dataset(X_test_tfidf, label=y_test_encoded, reference=train_data)\n",
    "models['LightGBM'] = lgb.train(\n",
    "    params, train_data, num_boost_round=1000, valid_sets=[train_data, test_data],\n",
    "    valid_names=['train', 'test'], callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    ")\n",
    "y_pred = (models['LightGBM'].predict(X_test_tfidf, num_iteration=models['LightGBM'].best_iteration) >= 0.5).astype(int)\n",
    "accuracies['LightGBM'] = accuracy_score(y_test_encoded, y_pred)\n",
    "\n",
    "# Define other models\n",
    "model_definitions = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, criterion=\"entropy\"),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(criterion='entropy', max_depth=20, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'SVM': svm.SVC(kernel='linear', probability=True)\n",
    "}\n",
    "\n",
    "# Train and evaluate other models\n",
    "for model_name, model in model_definitions.items():\n",
    "    model.fit(X_train_tfidf, y_train_encoded)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracies[model_name] = accuracy_score(y_test_encoded, y_pred)\n",
    "    models[model_name] = model\n",
    "\n",
    "# Save models, vectorizer, and label encoder\n",
    "for model_name, model in models.items():\n",
    "    with open(f\"{model_name.replace(' ', '_').lower()}_model.pkl\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save accuracies for frontend display\n",
    "with open('model_accuracies.json', 'w') as f:\n",
    "    json.dump(accuracies, f)\n",
    "\n",
    "print(\"All models and vectorizer saved successfully.\")\n",
    "print(\"\\nModel Accuracies:\")\n",
    "for model, acc in accuracies.items():\n",
    "    print(f\"{model}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5b08d-c2bf-4b4f-89cf-91424532eabc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
